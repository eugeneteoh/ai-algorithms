{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eugeneteoh/ai-algorithms/blob/transformer/transformer/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RiVhHKos0vI"
      },
      "source": [
        "Reference:\n",
        "\n",
        "https://github.com/Atcold/pytorch-Deep-Learning/blob/master/15-transformer.ipynb\n",
        "\n",
        "https://nn.labml.ai/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeIMc2erv64Z"
      },
      "outputs": [],
      "source": [
        "%pip install torchdata torchtext pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PN2PSkBiT8TA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/eugene/miniconda3/envs/transformer/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-fyu7yTglSfW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-HByNhYJlbda"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model=512, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = self.d_v = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        A = Q @ K.transpose(2, 3)\n",
        "        A /= np.sqrt(self.d_k)\n",
        "        A = F.softmax(A, dim=-1)\n",
        "        attn = A @ V\n",
        "        return attn\n",
        "\n",
        "    def forward(self, X_q, X_k, X_v):\n",
        "        batch_size, seq_length, dim = X_q.shape\n",
        "\n",
        "        Q = self.W_q(X_q)\n",
        "        K = self.W_k(X_k)\n",
        "        V = self.W_v(X_v)\n",
        "\n",
        "        # Split heads\n",
        "        Q = Q.view(batch_size, self.num_heads, seq_length, self.d_k)\n",
        "        K = K.view(batch_size, self.num_heads, seq_length, self.d_k)\n",
        "        V = V.view(batch_size, self.num_heads, seq_length, self.d_v)\n",
        "\n",
        "        H_cat = self.scaled_dot_product_attention(Q, K, V)\n",
        "        H_cat = H_cat.view(batch_size, seq_length, dim)\n",
        "\n",
        "        out = self.W_o(H_cat)\n",
        "        return out\n",
        "\n",
        "mha = MultiHeadAttention()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cclUvbmLszOH"
      },
      "outputs": [],
      "source": [
        "test_K = torch.tensor(\n",
        "    [[10, 0, 0],\n",
        "     [ 0,10, 0],\n",
        "     [ 0, 0,10],\n",
        "     [ 0, 0,10]]\n",
        ").float()[None,None]\n",
        "\n",
        "test_V = torch.tensor(\n",
        "    [[   1,0,0],\n",
        "     [  10,0,0],\n",
        "     [ 100,5,0],\n",
        "     [1000,6,0]]\n",
        ").float()[None,None]\n",
        "\n",
        "test_Q = torch.tensor(\n",
        "    [[0, 10, 0]]\n",
        ").float()[None, None]\n",
        "\n",
        "test_X_k = torch.randn((1, 1, 512))\n",
        "test_X_v = torch.randn((1, 1, 512))\n",
        "test_X_q = torch.randn((1, 1, 512))\n",
        "\n",
        "# Test scaled_dot_product_attention shape\n",
        "output = mha.scaled_dot_product_attention(test_Q, test_K, test_V)\n",
        "assert test_Q.shape == output.shape\n",
        "\n",
        "# Test mha output shape\n",
        "output = mha(test_X_q, test_X_k, test_X_v)\n",
        "assert test_X_q.shape == output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bjDj52al9hS6"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, num_heads=8, ff_hidden_dim=2048):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden_dim, d_model)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layernorm1(x + self.mha(x, x, x))\n",
        "        out = self.layernorm2(out + self.ff(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7dkGQL3d7nha"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, d_model=512, vocab_size=10000, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
        "        self.register_buffer(\"positional_encodings\", self.get_positional_encoding(d_model, max_len))\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def get_positional_encoding(self, d_model, max_len):\n",
        "        encodings = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        two_i = torch.arange(0, d_model, 2)\n",
        "        denominator = 10000 ** (two_i / d_model)\n",
        "        div = position / denominator\n",
        "        encodings[:, 0::2] = torch.sin(div)\n",
        "        encodings[:, 1::2] = torch.cos(div)\n",
        "        encodings.requires_grad_ = False\n",
        "\n",
        "        return encodings\n",
        "\n",
        "    def forward(self, x):\n",
        "        # seq_length = x.shape[1]\n",
        "        # position_ids = torch.arange(seq_length, dtype=torch.long, device=x.device) # (max_seq_length)\n",
        "        # position_ids = position_ids.unsqueeze(0).expand_as(x)                      # (bs, max_seq_length)\n",
        "\n",
        "        word_embeddings = self.word_embeddings(x)\n",
        "\n",
        "        embeddings = word_embeddings + self.positional_encodings\n",
        "        \n",
        "        return self.layernorm(embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x4Yl2K014Z_N"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size=10000, max_seq_len=5000, num_layers=6, d_model=512, num_heads=8, ff_hidden_dim=2048):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_layer = Embedding(d_model=d_model, vocab_size=vocab_size, max_len=max_seq_len)\n",
        "        self.enc_layers = nn.Sequential(*[EncoderLayer(d_model=d_model, num_heads=num_heads, ff_hidden_dim=ff_hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding_layer(x)\n",
        "        return self.enc_layers(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Puch1PUO-WGv"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, num_outputs, vocab_size=10000, max_seq_len=5000, num_layers=6, d_model=512, num_heads=8, ff_hidden_dim=2048):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size=vocab_size, max_seq_len=max_seq_len, num_layers=num_layers, d_model=d_model, num_heads=num_heads, ff_hidden_dim=ff_hidden_dim\n",
        "        )\n",
        "        self.dense = nn.Linear(d_model, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x, _ = torch.max(x, dim=1)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "P7-FUUXZvv3X"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import AG_NEWS, IMDB\n",
        "from collections import Counter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.functional import truncate, to_tensor\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchtext.transforms as T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9shF8CGQCidT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/eugene/miniconda3/envs/transformer/lib/python3.9/site-packages/torch/utils/data/datapipes/utils/common.py:143: UserWarning: Lambda function is not supported by pickle, please use regular python function or functools.partial instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = get_tokenizer('basic_english')  \n",
        "train_iter = IMDB(split='train')\n",
        "test_iter = IMDB(split='test')\n",
        "counter = Counter()\n",
        "for (label, line) in train_iter:\n",
        "    counter.update(tokenizer(line))\n",
        "for (label, line) in test_iter:\n",
        "    counter.update(tokenizer(line))\n",
        "data_vocab = vocab(counter, min_freq = 1, specials=('\\<unk\\>', '\\<BOS\\>', '\\<EOS\\>', '\\<PAD\\>'))\n",
        "\n",
        "batch_size = 16\n",
        "max_seq_len = 256\n",
        "\n",
        "text_transform = T.Sequential(\n",
        "    T.VocabTransform(data_vocab),\n",
        "    T.Truncate(max_seq_len),\n",
        "    T.ToTensor(),\n",
        "    T.PadTransform(max_seq_len, 1),\n",
        ")\n",
        "text_pipeline = lambda x: text_transform(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1\n",
        "apply_transform = lambda x: (label_pipeline(x[0]), text_pipeline(x[1]))\n",
        "\n",
        "train_iter = train_iter.map(apply_transform)\n",
        "train_iter = train_iter.batch(batch_size)\n",
        "train_iter = train_iter.rows2columnar([\"target\", \"token_ids\"])\n",
        "train_loader = DataLoader(train_iter, batch_size=None)\n",
        "\n",
        "test_iter = test_iter.map(apply_transform)\n",
        "test_iter = test_iter.batch(batch_size)\n",
        "test_iter = test_iter.rows2columnar([\"target\", \"token_ids\"])\n",
        "test_loader = DataLoader(test_iter, batch_size=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t5p8pXwY9DPp"
      },
      "outputs": [],
      "source": [
        "classifier = TransformerClassifier(num_outputs=2, vocab_size=len(data_vocab), max_seq_len=max_seq_len).to(device)\n",
        "\n",
        "for i, batch in enumerate(train_loader):\n",
        "    targets = torch.as_tensor(batch[\"target\"], device=device)\n",
        "    token_ids = torch.stack(batch[\"token_ids\"]).to(device)\n",
        "\n",
        "    out = classifier(token_ids)\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    # print(preds)\n",
        "    break\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "StDiz2reAyjQ"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from torchmetrics.functional.classification import binary_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "C_c_g-FLEhAj"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifierLT(pl.LightningModule):\n",
        "    def __init__(self, num_outputs, vocab_size=10000, max_seq_len=5000, num_layers=6, d_model=512, num_heads=8, ff_hidden_dim=2048):\n",
        "        super().__init__()\n",
        "        self.classifier = TransformerClassifier(\n",
        "            num_outputs=num_outputs,\n",
        "            vocab_size=vocab_size,\n",
        "            max_seq_len=max_seq_len,\n",
        "            num_layers=num_layers,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            ff_hidden_dim=ff_hidden_dim\n",
        "        )\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        targets = torch.as_tensor(batch[\"target\"], device=self.device)\n",
        "        token_ids = torch.stack(batch[\"token_ids\"]).to(self.device)\n",
        "        \n",
        "        \n",
        "        out = self.classifier(token_ids)\n",
        "\n",
        "        loss = F.cross_entropy(out, targets)\n",
        "\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        targets = torch.as_tensor(batch[\"target\"], device=self.device)\n",
        "        token_ids = torch.stack(batch[\"token_ids\"]).to(self.device)\n",
        "\n",
        "        out = self.classifier(token_ids)\n",
        "        preds = torch.argmax(out, dim=1)\n",
        "\n",
        "        test_loss = F.cross_entropy(out, targets)\n",
        "        test_acc = binary_accuracy(preds, targets)\n",
        "        self.log(\"test_loss\", test_loss)\n",
        "        self.log(\"accuracy\", test_acc)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "classifier = TransformerClassifierLT(num_outputs=2, vocab_size=len(data_vocab), max_seq_len=max_seq_len, num_layers=1, d_model=16, num_heads=2, ff_hidden_dim=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "P3jYv-mrFUbo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name       | Type                  | Params\n",
            "-----------------------------------------------------\n",
            "0 | classifier | TransformerClassifier | 2.4 M \n",
            "-----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.491     Total estimated model params size (MB)\n",
            "/Users/eugene/miniconda3/envs/transformer/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: : 1563it [03:06,  8.40it/s, loss=0.000214, v_num=1]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: : 1563it [03:06,  8.39it/s, loss=0.000214, v_num=1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/eugene/miniconda3/envs/transformer/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0: : 1563it [00:55, 27.98it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        accuracy            0.5001599192619324\n",
            "        test_loss            4.225028991699219\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'test_loss': 4.225028991699219, 'accuracy': 0.5001599192619324}]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    # limit_train_batches=100,\n",
        "    max_epochs=1,\n",
        "    # accelerator=\"gpu\",\n",
        "    # devices=1\n",
        ")\n",
        "num_workers = 2\n",
        "trainer.fit(model=classifier, train_dataloaders=train_loader)\n",
        "trainer.test(model=classifier, dataloaders=test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMp0gnCTQ8g5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNtOz3iZc4VXatq2LpY1XFE",
      "collapsed_sections": [],
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "transformer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "448d2cd31a28ee1d00350308051c5f1fc843643c229bd8b7366a65806baabfb6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
